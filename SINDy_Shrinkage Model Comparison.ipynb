{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Packages\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import glob\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateData(t, k, u, error):\n",
    "    \n",
    "    dt = t[1] - t[0]\n",
    "    ut = np.zeros(u.shape)\n",
    "\n",
    "    #compute derivative\n",
    "    for i in np.arange(1,len(u)-1):\n",
    "        ut[i] = (u[i+1] - u[i-1])/(2*dt)\n",
    "    ut[0] = (u[1] - u[0])/dt\n",
    "    ut[-1] = (u[-1] - u[-2])/dt\n",
    "\n",
    "    variables = [t[:,np.newaxis],u[:,np.newaxis],ut[:,np.newaxis]]\n",
    "    variable_names = ['t','u','u_t']\n",
    "    \n",
    "    variables = [t[:,np.newaxis],u[:,np.newaxis],ut[:,np.newaxis]]\n",
    "    \n",
    "    #U values\n",
    "    U = copy.deepcopy(variables[1])\n",
    "\n",
    "    for i in range(len(U)):\n",
    "        U[i] = U[i] + random.normal(loc=0, scale=error)\n",
    "    #Ut vector\n",
    "    Ut = copy.deepcopy(ut)\n",
    "\n",
    "    for i in range(len(Ut)):\n",
    "        Ut[i] = Ut[i] + random.normal(loc=0, scale=error)\n",
    "        \n",
    "    description = ['u^0','u^1','u^2','u^3','u^4', 'exp(u)']\n",
    "    \n",
    "    #print \"library = \" + str(description)\n",
    "    \n",
    "    #plt.figure()\n",
    "    #plt.plot(t,u,'blue')\n",
    "    #plt.plot(t,ut,'r')\n",
    "    #plt.scatter(t,U,s=3,facecolors='none', edgecolors='b')\n",
    "    #plt.scatter(t,Ut,s=3,facecolors='none', edgecolors='r')\n",
    "    #plt.show()\n",
    "    \n",
    "    Theta = np.hstack((np.ones(U.shape),U ,U**2,U**3, U**4, np.exp(U)))\n",
    "    Theta = preprocessing.scale(Theta)\n",
    "    Theta[:, 0] = 1\n",
    "    dim = len(description)\n",
    "    return Theta, Ut, len(description), description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Ridge_Accuracy = []\n",
    "Lasso_Accuracy = []\n",
    "relaxedLasso_Accuracy = []\n",
    "total_size = 10\n",
    "\n",
    "# Change to whatever you want to iterate on\n",
    "for error in np.linspace(0.001,0.1,10):\n",
    "    for iterating in range(total_size):\n",
    "        Ridge_Acc = 0.0\n",
    "        Lasso_Acc = 0.0\n",
    "        relaxedLasso_Acc = 0.0\n",
    "        t = np.linspace(0,30,100)#random.random_integers(0,30,50)\n",
    "        u = np.exp(k*t)/(10 + np.exp(k*t))\n",
    "        k = 0.45\n",
    "\n",
    "        #print \"k = \" + str(k)\n",
    "        #print \"sample size = \" + str(len(t))\n",
    "        #print \"deviation of noise = \" + str(error)\n",
    "\n",
    "        Theta, Ut, dim, description = generateData(t, k, u, error)\n",
    "\n",
    "        #print \"Full regression predicts the ODE model:\"\n",
    "        #print linear_model.LinearRegression().fit(Theta,Ut).coef_\n",
    "\n",
    "        #print \"\"\n",
    "        #print \"Bootstrap Model Selection - regression predicts the ODE model:\"\n",
    "        #bootSelect(Theta, Ut, dim = dim, description = description, reg = linear_model.LassoCV(alphas = np.linspace(0.001, 0.01, 100), cv = 10))\n",
    "\n",
    "        #print \"\"\n",
    "        #print \"Ridge + Boot - regression predicts the ODE model:\"\n",
    "        #Ridge_Boot(Theta, Ut, dim = dim, description = description)\n",
    "\n",
    "        #print \"\"\n",
    "        #print \"Ridge + CV - regression predicts the ODE model:\"\n",
    "        #1 loop, best of 3: 1.9 s per loop %timeit\n",
    "        if Ridge_CV(Theta, Ut, dim = dim, description = description, InCr = \"BIC\") == ['u^1', 'u^2']:\n",
    "            Ridge_Acc = Ridge_Acc + 1\n",
    "\n",
    "        #print \"\"\n",
    "        #print \"Lasso + CV - regression predicts the ODE model:\"\n",
    "        # 10 loops, best of 3: 126 ms per loop %timeit \n",
    "        if Ridge_CV(Theta, Ut, dim = dim, model = linear_model.LassoCV, description = description, InCr = \"BIC\") == ['u^1', 'u^2']:\n",
    "            Lasso_Acc = Lasso_Acc + 1\n",
    "\n",
    "        #print \"\"\n",
    "        #print \"ElasticNet + CV - regression predicts the ODE model:\"\n",
    "        #Ridge_CV(Theta, Ut, dim = dim, model = linear_model.ElasticNetCV, description = description, InCr = \"BIC\")\n",
    "\n",
    "\n",
    "        #print \"\"\n",
    "        #print \"Relaxed Lasso - regression predicts the ODE model:\"\n",
    "        #1 loop, best of 3: 286 ms per loop %timeit\n",
    "        if relaxedLasso(Theta, Ut, dim = dim, description = description, InCr = \"BIC\") == ['u^1', 'u^2']:\n",
    "            relaxedLasso_Acc = relaxedLasso_Acc + 1\n",
    "\n",
    "    Ridge_Acc = Ridge_Acc/total_size\n",
    "    Lasso_Acc = Lasso_Acc/total_size\n",
    "    relaxedLasso_Acc = relaxedLasso_Acc/total_size\n",
    "    Ridge_Accuracy.append(Ridge_Acc)\n",
    "    Lasso_Accuracy.append(Ridge_Acc)\n",
    "    relaxedLasso_Accuracy.append(Ridge_Acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ridge_Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ridge_Boot(Theta, Ut, dim, description):\n",
    "    ##Cross Validation\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.linear_model import RidgeCV\n",
    "\n",
    "    best_lambda_tol = None\n",
    "    best_d_tol = None\n",
    "    best_meanOfResiduals = 100000000\n",
    "    best_pde = None\n",
    "    \n",
    "    #clf = RidgeCV(alphas=np.linspace(0.000001, 0.00001, 50)).fit(Theta, Ut)\n",
    "    return bootSelect(Theta, Ut, dim, description, reg = RidgeCV(alphas=np.linspace(0.001, 0.001, 50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootSelect(Theta, Ut, dim, description, Bag = 100, reg = linear_model.LinearRegression()):\n",
    "    from sklearn import linear_model\n",
    "    from sklearn.utils import resample\n",
    "    boot_reg_coef = []\n",
    "    for i in range(Bag):\n",
    "        X, y = resample(Theta, Ut, replace=True)\n",
    "        reg.fit(X, y)\n",
    "        boot_reg_coef.append(reg.coef_)\n",
    "    boot_reg_coef = np.array(boot_reg_coef)\n",
    "    #print(boot_reg_coef)\n",
    "    \n",
    "    for i in range(dim):\n",
    "        coef = boot_reg_coef[:, i]\n",
    "        coef = np.sort(coef)\n",
    "        print description[i] + \":[\" + str(coef[int(0.10 * Bag)]) + \" , \" + str(coef[int(0.90 * Bag)]) + \"]\"\n",
    "    \n",
    "    reg_coef = []\n",
    "    k = len(boot_reg_coef)\n",
    "    for i in range(dim):\n",
    "        coef = boot_reg_coef[:, i]\n",
    "        coef = np.sort(coef)\n",
    "        if coef[int(0.10 * Bag)] < 0 and coef[int(0.90 * Bag)] > 0:\n",
    "            reg_coef.append(0)\n",
    "        else:\n",
    "            reg_coef.append(np.mean(coef))\n",
    "    return bestSubsetSelection(Theta, Ut, reg_coef, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ridge_CV(Theta, Ut, dim, description, model = linear_model.RidgeCV, InCr = \"AIC\"):\n",
    "\n",
    "    best_lambda_tol = None\n",
    "    best_d_tol = None\n",
    "    best_sumOfResiduals = 100000000\n",
    "    best_pde = []\n",
    "    \n",
    "    RidgeResult = model(alphas = np.linspace(0.000001, 0.01, 100), cv = 10).fit(Theta, Ut)\n",
    "    best_lambda_tol = RidgeResult.alpha_\n",
    "    #if model == linear_model.RidgeCV:\n",
    "    #    RidgeResult.coef_ = RidgeResult.coef_[0]\n",
    "    \n",
    "    sortedCoef = np.sort(abs(RidgeResult.coef_))\n",
    "    ## mini = index of the sortedCoef\n",
    "    mini = 0\n",
    "    #Swapping unnecessary Betas into zero\n",
    "    # Trying rule of thumb as sortedCoef[dim - 1]/3\n",
    "    for d_tol in np.linspace(0, sortedCoef[dim - 1]/2, 50):\n",
    "        if sortedCoef[mini] >= d_tol:\n",
    "            # Skip\n",
    "            continue\n",
    "        for j in range(dim):\n",
    "            curCoef = abs(RidgeResult.coef_[j])\n",
    "            if curCoef < d_tol:\n",
    "                # update mini\n",
    "                while sortedCoef[mini] <= curCoef:\n",
    "                    mini = mini + 1\n",
    "                RidgeResult.coef_[j] = 0\n",
    "        \n",
    "        kf = KFold(n_splits=10, shuffle=True)\n",
    "        residualList = []\n",
    "        for train, test in kf.split(Ut):\n",
    "            sumOfResiduals = np.linalg.norm(Ut[test] - Theta[test].dot(np.transpose(RidgeResult.coef_)))\n",
    "            residualList.append(sumOfResiduals)\n",
    "        sumOfResiduals = np.sum(residualList)\n",
    "        if best_sumOfResiduals > sumOfResiduals:\n",
    "            best_d_tol = d_tol\n",
    "            best_pde = RidgeResult.coef_\n",
    "            best_sumOfResiduals = sumOfResiduals\n",
    "    #print \"best_lambda_tol = \" + str(best_lambda_tol)\n",
    "    #print \"best_d_tol = \" + str(best_d_tol)\n",
    "    #print \"best_pde = \" + str(RidgeResult.coef_)\n",
    "    #print \"best_sumOfResiduals = \" + str(best_sumOfResiduals)\n",
    "    return bestSubsetSelection(Theta, Ut, best_pde, description, InCr = InCr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestSubsetSelection(Theta, Ut, best_pde, description, InCr = \"AIC\"):\n",
    "    Theta = np.transpose(Theta)\n",
    "    best_IC = 100000000\n",
    "    best_reg = []\n",
    "    best_subset = []\n",
    "    #sublib = [non-zero indices]\n",
    "    sublib = []\n",
    "    for i in range(len(best_pde)):\n",
    "        if best_pde[i] != 0:\n",
    "            sublib.append(i)\n",
    "    \n",
    "    Theta_new = copy.deepcopy(Theta[sublib])\n",
    "    \n",
    "    # Best Subset Selection ** expensive\n",
    "    sublib_size = len(sublib)\n",
    "    description = np.array(description)\n",
    "\n",
    "    for i in range(1, 2**sublib_size):\n",
    "        subset = []\n",
    "        j = i\n",
    "        cur = sublib_size - 1\n",
    "        while j:\n",
    "            if j & 1:\n",
    "                subset.append(cur)\n",
    "            cur = cur - 1\n",
    "            j = j >> 1\n",
    "        \n",
    "        reg, IC = getIC(np.transpose(Theta_new[subset]), Ut, len(subset), InCr = InCr)\n",
    "        if IC < best_IC:\n",
    "            best_IC = IC\n",
    "            best_reg = reg\n",
    "            best_subset = subset\n",
    "            \n",
    "    model_size = len(best_reg)\n",
    "    if not model_size:\n",
    "        #print \"Null Model\"\n",
    "        return\n",
    "    \n",
    "    #print \"Best \" + InCr + \" = \" + str(IC)\n",
    "    #print \"Ut = \"\n",
    "    #for i in range(model_size - 1):\n",
    "    #    print str(best_reg[i]) + \" \" + str(description[sublib][best_subset][i]) + \" + \"\n",
    "    #print str(best_reg[model_size - 1]) + \" \" + str(description[sublib][best_subset][model_size - 1])\n",
    "    reg_result = []\n",
    "    for i in range(model_size - 1, -1, -1):\n",
    "        reg_result.append(description[sublib][best_subset][i])\n",
    "    return reg_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsetData(Theta, best_pde, description):\n",
    "    Theta = np.transpose(Theta)\n",
    "    best_reg = []\n",
    "    best_subset = []\n",
    "    #sublib = [non-zero indices]\n",
    "    sublib = []\n",
    "    for i in range(len(best_pde)):\n",
    "        if best_pde[i] == 0:\n",
    "            sublib.append(i)\n",
    "    Theta_new = copy.deepcopy(Theta)\n",
    "    Theta_new[sublib] = 0\n",
    "    description = np.array(description)\n",
    "    return np.transpose(Theta_new), description[sublib]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIC(Theta, Ut, p, InCr = \"AIC\"):\n",
    "    from sklearn import linear_model\n",
    "    reg = linear_model.LinearRegression()\n",
    "    reg.fit(Theta, Ut)\n",
    "    n = len(Ut)\n",
    "    sumOfResiduals = np.linalg.norm(Ut - Theta.dot(np.transpose(reg.coef_)))\n",
    "    ICDict = {\"AIC\": n*np.log(sumOfResiduals)+2*p,\n",
    "              \"BIC\": n*np.log(sumOfResiduals/n)+p*np.log(n)}\n",
    "    return reg.coef_, ICDict[InCr]\n",
    "# \"AIC\": n*np.log(sumOfResiduals)+2*p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relaxedLasso(Theta, Ut, dim, description, InCr = \"AIC\"):\n",
    "    best_coef = linear_model.LassoCV(alphas = np.linspace(0.0001, 0.01, 100), cv = 10).fit(Theta, Ut).coef_\n",
    "    Theta_new, description_new = subsetData(Theta, best_coef, description)\n",
    "    second_coef = linear_model.LassoCV(alphas = np.linspace(0.00001, 0.001, 100), cv = 10).fit(Theta_new, Ut).coef_\n",
    "    return bestSubsetSelection(Theta, Ut, second_coef, description, InCr = InCr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
